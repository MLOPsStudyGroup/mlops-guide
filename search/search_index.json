{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLOps Guide","text":"<p>This site is intended to be a MLOps Guide to help projects and companies to build more reliable MLOps environment. This guide should contemplate the theory behind MLOps and an implementation that should fit for most use cases. Made by Arthur Olga, Gabriel Monteiro, Guilherme Leite and Vinicius Lima</p>"},{"location":"#what-is-mlops","title":"What is MLOps?","text":"<p>MLOps is a methodology of operation that aims to facilitate the process of bringing an experimental Machine Learning model into production and maintaining it efficiently. MLOps focus on bringing the methodology of DevOps used in the software industry to the Machine Learning model lifecycle. In that way we can define some of the main features of a MLOPs project:</p> <ul> <li>Data and Model Versioning</li> <li>Feature Management and Storing</li> <li>Automation of Pipelines and Processes</li> <li>CI/CD for Machine Learning</li> <li>Continuous Monitoring of Models</li> </ul>"},{"location":"#what-is-contemplated-on-this-guide","title":"What is Contemplated on This Guide?","text":"<ul> <li>Introduction to MLOps Concepts</li> <li>Tutorial for Building a MLOps Environment</li> </ul>"},{"location":"#mlops-environment","title":"MLOps Environment","text":"<p>This video shows how an example of workflow with a complete MLOps project. This exact project can be found here and is an example end-to-end made for this guide.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The following diagram shows the complete MLOps flow used on the tutorial. Since the guide is modular, a team can choose to swap tools at any point due to project preferences and use cases.</p> <p></p>"},{"location":"#project-tools","title":"Project Tools","text":"<p>The main tools discussed in the guide are shown in the following table.</p> <p> Tools Function Developer License IBM Watson ML Deploying model as API IBM Proprietary IBM Watson OpenScale Monitoring Model in production IBM Proprietary DVC Data and Model Versioning Iterative Apache License 2.0 CML Pipeline Automation Iterative Apache License 2.0 Terraform Setups IBM infrastructure with script HashiCorp Mozilla Public License v2.0 Github Code versioning Github Proprietary Github Actions CI/CD Automation Github Proprietary Pytest Python script testing Pytest-dev MIT Pre-commit Running tests on local commit Pre-commit MIT Cookiecutter Creating folder structure and files Cookiecutter BSD 3-Clause <p></p>"},{"location":"#next","title":"Next","text":"<p>MLOps Theory</p> <p>\ud83d\udcda Learn More About MLOps Theory</p> <p>Tip</p> <p>It is recommended that you learn about the theory before implementing MLOps into your project</p> <p>Implementation Guide</p> <p>\ud83d\udcc3 Follow the Tutorial to Start a Project</p>"},{"location":"glossary/","title":"Contents","text":"<p>A short description of each topic or subtopic.</p> <ul> <li> <p>MLOps Theory: A brief introduction to the main Principles of MLOps: Data and Model Versioning, Feature Management and Storing, Automation of Pipelines and Processes, CI/CD for Machine Learning and Continuous Monitoring of Models. As well as Common Tools used to address each of those points.</p> </li> <li> <p>Implementations Guide</p> <ul> <li>Introduction<ul> <li>Tools and Project Structure: Introduction to the project structure and tools that will be used.</li> <li>Starting a New Project with Cookiecutter: Introduction to  Cookiecutter and how to create a new project based on our template.</li> </ul> </li> <li> <p>Environment</p> <ul> <li>Setting Up the IBM Environment with Terraform: Using Terraform to set up the IBM Environment via code.</li> <li>Managing the deployment space: How to manage IBM's tools with Terraform.</li> </ul> </li> <li> <p>Versioning</p> <ul> <li>What is DVC?: Introduction to DVC, installation and how to setup remote storage.</li> <li>Data Versioning: Working with DVC to version data and models.</li> <li>Working with Pipelines: Creating and reproducing pipelines for training and evaluating models.</li> </ul> </li> <li> <p>Deployment</p> <ul> <li>Deployment with Watson Machine Learning: Using Watson ML API to deploy models as an online web service.</li> </ul> </li> <li> <p>CI/CD for Machine Learning</p> <ul> <li>Continuous Integration with CML and Github Actions: Introduction to CML and GitHub Actions and how to create automatic testing and reportng workflows.</li> <li>Continous Delivery with CML, Github Actions and Watson ML: Using CML and GitHub Actions create automatic deployments for every new release.</li> </ul> </li> <li> <p>Monitoring</p> <ul> <li>Monitoring with IBM OpenScale: Setting up OpenScale environment, creating monitors, evaluating the model and explaining predictions.</li> </ul> </li> <li> <p>Project Workflow</p> <ul> <li>Project Workflow: Demonstrating how to use the complete pipeline in a development cycle we created with commands and videos.</li> </ul> </li> </ul> </li> </ul>"},{"location":"CICD/cml_deploy/","title":"Continous Delivery with CML, Github Actions and Watson ML","text":"<p>To deploy or update the deployment without the need to manually do so by the Watson ML UI or by running a script, we use GItHub Actions to run a workflow everytime we make a new release and then deploy the model that way. </p>"},{"location":"CICD/cml_deploy/#creating-a-release","title":"Creating a Release","text":"<ol> <li>On GitHub, go to the main page of the repository.</li> <li> <p>To the right of the page, click on <code>Releases</code> or <code>Latest release</code>.   </p> </li> <li> <p>Click <code>Draft a new release</code> on the top right.   </p> </li> <li> <p>Type the version of the new release.</p> </li> <li>Click on <code>Publish release</code>.    </li> </ol> <p>After that, Actions will trigger and the model on Watson ML, next we will explain how to implement this workflow. For more information regarding releases with GitHub, refer to this article.</p>"},{"location":"CICD/cml_deploy/#git-actions","title":"Git Actions","text":"<p>The workflow downloads the data from dvc the uses credentials, to understand all the previous steps, refer to the last section where we explained step by step what is CML and how to implement workflows. The complete workflow can be found here.</p> <p>First we set the workflow to trigger every time a new release is created: <pre><code>name: model-deploy-on-release\non:\n  release:\n    types: \n      - 'created'\n</code></pre> Then we execute the following steps: <pre><code>run: |\n    # Install requirements\n    pip install -r requirements.txt\n    # Pull data &amp; run-cache from S3 and reproduce pipeline\n    dvc pull --run-cache\n    dvc repro\n    # Decrypt credentials file\n    gpg --quiet --batch --yes --decrypt --passphrase=\"$CRED_SECRET\" --output credentials.yaml credentials.yaml.gpg\n    # Check if there is a deployment already, if positive update it, otherwise deploys it for the first time\n    ./src/scripts/Scripts/git_release_pipeline.sh \n</code></pre></p> <p>Installing requirements <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>Pull the versioned data and reproduce the full pipeline of training and evaluation <pre><code>dvc pull --run-cache\ndvc repro\n</code></pre></p> <p>Decrypting Credentials File <pre><code>    gpg --quiet --batch --yes --decrypt --passphrase=\"$CRED_SECRET\" --output credentials.yaml credentials.yaml.gpg\n</code></pre></p> <p>Run a script to Deploy or Update the Deployment <pre><code>./src/scripts/Scripts/git_release_pipeline.sh \n</code></pre> This is the following bash script:</p> <pre><code>#!/bin/sh\n\nif  ! python3 ./src/scripts/Pipelines/git_release_pipeline.py ./\nthen \n    echo \"      Model already has been deployed, updating it\"\n    python3 ./src/scripts/Pipelines/model_update_pipeline.py ./models/model.joblib ./ ./credentials.yaml\n    python3 ./src/scripts/Pipelines/model_update_deployment_pipeline.py ./ ./credentials.yaml\nelse    \n    echo \"      Deploying model for the first time\" \n    python3 ./src/scripts/Pipelines/model_deploy_pipeline.py ./models/model.joblib ./ ./credentials.yaml\nfi\n</code></pre> <p>First we check if there is a model UID in the <code>metadata.yaml</code>. If positive, we consider the model has already been deployed, and the we run scripts to update the model and the deployment. </p> <p>To find more details on how we use scripts to deploy the model on Watson Machine Learning you can go to this page from our guide.</p>"},{"location":"CICD/cml_testing/","title":"Continuous Integration with CML and Github Actions","text":""},{"location":"CICD/cml_testing/#what-is-cml","title":"What is CML?","text":"<p>CML, which stands by Continuous Machine Learning, it's an open-source library focused on delivering CI/CD for machine learning projects. Its principles includes:  GitFlow: Using Git workflow for managing experiments alongside DVC versioning data and models. Auto reports for experiments: CML can generate reports in pull requests with metrics and plots helping the team to make informed and data-driven decisions Technology Agnostics: Build pipelines with Github or Gitlab and run experiments with any cloud service. Since we are using Github as our Git repository, Github Actions will be used to set up CML. Github Actions is managed by Github, so there is no need to worry about scale and operate the infrastructure just as other tools like Jenkins.</p>"},{"location":"CICD/cml_testing/#testing-with-github-actions","title":"Testing With Github Actions","text":"<p>First, we will make sure our tests created at the Testing with Pytest and Black section are being executed every time that there is a new push to the repository on Github. This important in order to achieve redundancy in testing the project, and avoid making sure the code runs without errors on any environment and not just the developer's computer.</p> <p>To do this is very simple. If you used cookiecutter you should already have a file named test_on_push.yaml at <code>.github/workflows/</code> folder. The content of the file should be:</p> <pre><code>name: Python Package and Test\n\n    on: [push]\n\n    jobs:\n        build:\n\n        runs-on: ubuntu-latest\n        strategy:\n        matrix:\n            python-version: [3.6]\n\n        steps:\n        - uses: actions/checkout@v2\n        - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n        python-version: ${{ matrix.python-version }}\n        - name: Install dependencies\n        run: |\n        python -m pip install --upgrade pip\n        pip install pytest black\n        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n        - name: Test with pytest\n        run: |\n        pytest\n        - name: Python Black\n        run: |\n        black . --check\n</code></pre> <p>Now, we should take a look into those commands.</p> <p>Choosing when this action will run <pre><code>on: [push]\n</code></pre> Makes it run every time there is a push on the repository</p> <p>Setting up a Github Instance to run it <pre><code>runs-on: ubuntu-latest\n</code></pre> Github will setup a free Ubuntu instance for us, using the latest official release.</p> <p>Chosing the right version of Python <pre><code>matrix:\n    python-version: [3.6]\n</code></pre></p> <p>Installing test requirements <pre><code>python -m pip install --upgrade pip\npip install pytest black\n</code></pre></p> <p>Installing project requirements <pre><code>if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n</code></pre></p> <p>Running Pytest <pre><code>- name: Test with pytest\n    run: |\n    pytest\n</code></pre></p> <p>Running Black check <pre><code>- name: Python Black\n     run: |\n     black . --check\n</code></pre> This checks that every Python file is formatted according to Black.</p> <p>If all tests pass the commit pushed to the repository will receive a green check show it has no errors. </p> <p></p>"},{"location":"CICD/cml_testing/#pipeline-test","title":"Pipeline Test","text":"<p>After setting up our test on push, let's focus on reproducing our pipeline on push and generate an automated report in pull requests to compare the experiment with the model at the main branch.</p>"},{"location":"CICD/cml_testing/#configuring-credentials","title":"Configuring Credentials","text":"<p>To reproduce our experiment pipeline, we need to start pulling our data versioned by DVC. On the other hand, Github Actions will execute our pipeline inside a container pre-configured by CML that doesn't have our IBM credentials to pull the data from IBM COS. So, to solve this, let's configure our IBM credentials in Github Secrets.</p> <p>Github Secrets is a repository tool that encrypts credentials to be used as environment variables in the project.</p> <ul> <li>Go to the repository settings</li> </ul> <p></p> <ul> <li>On the left menu, click on 'Secrets'</li> </ul> <p></p> <ul> <li>Click on 'New repository secret'</li> </ul> <p></p> <ul> <li>Add both <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code></li> </ul> <p></p>"},{"location":"CICD/cml_testing/#adding-train-and-evaluate-workflow","title":"Adding Train and Evaluate Workflow","text":"<p>Just like we did at the Testing Setup, let's create a file named train_evaluate.yaml at the <code>.github/workflows/</code> folder, which content should be:</p> <pre><code>name: model-training-evaluate\n    on: [push]\n    jobs:\n     run:\n     runs-on: [ubuntu-latest]\n     container: docker://dvcorg/cml-py3:latest\n     steps:\n     - uses: actions/checkout@v2\n     - name: 'Train and Evaluate model'\n     shell: bash\n     env:\n     repo_token: ${{ secrets.GITHUB_TOKEN }}\n     AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n     AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n     run: |\n     # Install requirements\n     pip install -r requirements.txt\n\n     # Pull data &amp; run-cache from IBM COS and reproduce pipeline\n     dvc pull --run-cache\n     dvc repro\n\n     # Report metrics\n     echo \"## Metrics\" &gt;&gt; report.md\n     git fetch --prune\n     dvc metrics diff master --show-md &gt;&gt; report.md\n\n     # Publish ROC Curve and \n     echo -e \"## Plots\\n### ROC Curve\" &gt;&gt; report.md\n     cml-publish ./results/roc_curve.png --md &gt;&gt; report.md\n     echo -e \"\\n### Precision and Recall Curve\" &gt;&gt; report.md\n     cml-publish ./results/precision_recall_curve.png --md &gt;&gt; report.md\n     cml-send-comment report.md\n</code></pre> <p>Let's dig into each command:</p> <p>Setting up a CML pre-configured container <pre><code>container: docker://dvcorg/cml-py3:latest\n</code></pre></p> <p>Setting up environment credentials for IBM COS <pre><code>env: repo_token: ${{ secrets.GITHUB_TOKEN }} \nAWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\nAWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY}}\n</code></pre></p> <p>Installing requirements <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>Pull the versioned data and reproduce the full pipeline of training and evaluation <pre><code>dvc pull --run-cache\ndvc repro\n</code></pre></p> <p>Formatting report section tittles <pre><code>echo \"## Metrics\" &gt;&gt; report.md\necho -e \"## Plots\\n### ROC Curve\" &gt;&gt; report.md\necho -e \"\\n### Precision and Recall Curve\" &gt;&gt; report.md\n</code></pre></p> <p>Comparing metrics and publishing it to the report <pre><code>dvc metrics diff master --show-md &gt;&gt; report.md\n</code></pre></p> <p>Publishing figures from the experiment to the report <pre><code>cml-publish ./results/roc_curve.png --md &gt;&gt; report.md\ncml-publish ./results/precision_recall_curve.png --md &gt;&gt; report.md\n</code></pre></p> <p>Return the final report formatted as a comment on the Commit or Pull Request <pre><code>cml-send-comment report.md\n</code></pre></p> <p>The Report should look like the following: (.zoom)</p>"},{"location":"CICD/tests/","title":"Implementing Tests","text":""},{"location":"CICD/tests/#why-testing-your-project","title":"Why Testing Your Project?","text":"<p>In the software development industry it has become a good practice to test your code and try achieving the most possible coverage. This practice helps with making a project that is maintainable, reliable and avoids problems like shipping new versions that break old features. It is important to follow the same practice on the MLOps spectrum by testing the code, the model and even the data. Making tests is somewhat very particular to the project, so this section will show a simple implementation of a testing pipeline using Pytest and Pre-commit, which should later be filled with tests that comprehend the specific project needs.</p>"},{"location":"CICD/tests/#types-of-testing","title":"Types of testing","text":"<p>It is important to notice that many projects will have different needs in terms of testing, so this division may not be precise for all projects, but in broad terms the main types of testing in MLOps are:</p> <ul> <li> <p>Software Testing: Comprehends tests that make sure the code follows the project requirements. This is the type of tests normally implemented in DevOps, such as Unit Testing, Integration Testing, System Testing and Acceptance Testing.</p> </li> <li> <p>Model Testing: Comprehends tests that define that the model is working fine, such as testing that it can be trained, or that it can obtain a minimum score at some evaluation.</p> </li> <li> <p>Data Testing: Comprehends tests that check for data correctness. It is heavily dependant on the project requirements and can be focused on securing that the analyzed dataset follows a schema, contains enough data, and others. Many thing can be tested here depending on the necessities of the team.</p> </li> </ul>"},{"location":"CICD/tests/#using-pytest","title":"Using Pytest","text":"<p>First, we will be using a Python package called Pytest, which is a very popular choice for regular Software testing and can be used to implement all sorts of tests. We are going to build a simple a Pytest file that shows an example of testing the whole project and the team can later add more tests specific for their use cases.</p> <p>In the src/tests/ folder you should see a Python file with a custom name with this content:</p> <pre><code>import pytest\n\n\ndef capital_case(x):\n    return x.capitalize()\n\n\ndef test_capital_case():\n    assert capital_case(\"semaphore\") == \"Semaphore\"\n</code></pre> <p>This is a simple file that imports the Pytest package and shows a simple test. <code>capital_case()</code> is a function that could be defined in any place of the entire project, and <code>test_capital_case()</code> is a test function, which means it should be used to test the capital case function. To run all tests:</p> <pre><code>pytest\n</code></pre> <p>By running <code>pytest</code> on the command line Python finds all the files that have a function starting with the <code>test_</code> prefix  and run them. This means that <code>capital_case()</code> is not run directly and is only running inside <code>test_capital_case()</code>.</p>"},{"location":"CICD/tests/#configuring-pre-commit","title":"Configuring Pre-commit","text":"<p>Pre-commit is a tool that enables us to use Git Hooks, a particular feature of Git itself, not necessarily Github or GitLab, that enables us to run scripts and commands when performing certain actions such as commit, push, pull, etc. This CLI tool enables us to create a Git Hook easily to test our code before each commit, making it harder for a developer to add code to the Github repository with errors.</p> <p>You should have a <code>.pre-commit-config.yaml</code> file at the root directory with:</p> <pre><code>--- \nrepos:\n-\n  repo: https://github.com/ambv/black\n  rev: 20.8b1\n  hooks: \n    - \n      id: black\n      language_version: python3\n\n-   repo: local\n    hooks:\n    -   id: python-tests\n        name: pytests\n        entry: pytest src/tests\n        language: python\n        additional_dependencies: [pre-commit, pytest, pandas, sklearn, matplotlib]\n        always_run: true\n        pass_filenames: false\n</code></pre> <p>This file can be used to configure a Git Hook that runs Pytest and Black Python formatter before each commit. </p> <p>So, to install this Git Hook use: <pre><code>pre-commit install\n</code></pre> Now, after you commit some changes in the repository you should see a message stating that all tests passed. If some of the tests didn't pass, the Git Hook won't let you commit until you fix the errors.</p> <pre><code>$ git commit -m \"Example commit\"\n\nblack....................................................................Passed\npytest-check.............................................................Passed\n</code></pre>"},{"location":"CICD/tests/#adding-software-testing","title":"Adding Software Testing","text":"<p>Now we are going to write some more tests for the project. One of the most necessary parts of testing in this project is on the Preprocess Pipeline, which comprehends the highest number of functions. So here we will be implementing a test the covers those functions, their ability to read and write files, and that a reduced dataset can be preprocessed with no problems.</p> <p>Create a folder called <code>preprocess/</code> inside <code>tests/</code>, and inside this folder a file called <code>test_preprocess.py</code> and another folder called <code>test_data/</code>, in which we are going to add a simple dataset file <code>testWeatherAUS.csv</code> with the first 2 lines of the real dataset.</p> <p>Now with this created, edit <code>tests/preprocess/test_preprocess.py</code> with some imports and an addition to path to be able to access the test dataset. </p> <pre><code>import io\nimport builtins\nimport pytest\nimport pandas as pd\nimport sys\nimport os\n\n# Parent Folder which contains test_data/\nsys.path.append(\n    os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\n)\n\n# Preprocess Python file which contains our functions to be tested\nimport preprocess_data\n\nFILE_NAME = \"testWeatherAUS\"\nDATA_PATH = (\n    os.path.dirname(os.path.realpath(__file__)) + \"/test_data/\" + FILE_NAME + \".csv\"\n)\nPROCESSED_DATA_PATH = (\n    os.path.dirname(os.path.realpath(__file__))\n    + \"/test_data/\"\n    + FILE_NAME\n    + \"_processed.csv\"\n)\n</code></pre> <p>Now, we are going to add unit testing for some of the functions inside <code>preprocess_data.py</code>.</p> <pre><code>def test_count_nulls_by_line():\n    # Tests function that counts number of nulls by line on a dataframe\n    data = pd.DataFrame([[0, 2], [0, 1], [6, None]])\n    assert preprocess_data.count_nulls_by_line(data).to_list() == [1, 0]\n\n\ndef test_null_percent():\n    # Tests function that gets the percentage of nulls by line on a dataframe\n    data = pd.DataFrame([[0, 2], [1, None]])\n    assert preprocess_data.null_percent_by_line(data).to_list() == [0.5, 0]\n</code></pre> <p>Now that we made some examples of unit testing, we should add tests that comprehend the full pipeline, in this case checking if it runs smoothly or returns an error. Pretty general test that can be used to avoid some basic mistakes. This test creates a <code>testWeatherAUS_processed.csv</code> file.</p> <p>This test is going to be important for the next tests, so we will add a mark of dependency, which is a Pytest feature that can be used to tell Python that a test function should be run before another. In this case, any function that marks this function as a dependency will run after this one finishes.</p> <pre><code>@pytest.mark.dependency()\ndef test_preprocess():\n    # Checks if running the preprocess function returns an error\n    preprocess_data.preprocess_data(DATA_PATH)\n</code></pre> <p>Adding tests that make sure the file is created and is possible to read, which comprehends some system errors. It is possible to see that these functions have marked dependencies, which will make them run after the functions marked.</p> <pre><code>@pytest.mark.dependency(depends=[\"test_preprocess\"])\ndef test_processed_file_created():\n    #  Checks if the processed file was created during test_preprocess() and is accessible\n    f = open(PROCESSED_DATA_PATH)\n\n\n@pytest.mark.dependency(depends=[\"test_processed_file_created\"])\ndef test_processed_file_format():\n    # Checks if the processed file is in  the correct format (.csv) and can be transformed in dataframe\n    try:\n        pd.read_csv(PROCESSED_DATA_PATH)\n    except:\n        raise RuntimeError(\"Unable to open \" + PROCESSED_DATA_PATH + \" as dataframe\")\n</code></pre> <p>Now that we have implemented our example of tests, we need to delete the file created, which is <code>test_data/testWeatherAUS_processed.csv</code>. A ligature is a feature of Pytest that makes it possible to run code before or after the tests, which are run in the <code>yield</code> call on the code.</p> <pre><code>@pytest.fixture(scope=\"session\", autouse=True)\ndef cleanup(request):\n    # Runs tests then cleans up the processed file\n    yield\n    try:\n        os.remove(PROCESSED_DATA_PATH)\n    except:\n        pass\n</code></pre> <p>Now, when you run a <code>pytest</code>, or <code>`git commit</code> which triggers pytest, all these tests should run.</p>"},{"location":"CICD/tests/#adding-more-test-files","title":"Adding More Test Files","text":"<p>It is important to note that it is very important to tests the functions that are being used in the scripts of model handling and data handling, such as the function <code>get_variables()</code> in the <code>model.py</code>. Se we are going to follow the same structure and create a file <code>tests/model/test_model.py</code>.</p> <p>In this case we would like to test a large variety of different inputs, so we will be using <code>pytest.mark.parametrize()</code> which enables us to choose a large amount of inputs and expected results.</p> <pre><code>import sys\nimport os\nimport pytest\nimport pandas as pd\n\n# Parent Folder\nsys.path.append(\n    os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\n)\n\n# Model Python file\nfrom model import get_variables\n\nFILE_NAME = \"testWeatherAUS\"\nPROCESSED_DATA_PATH = (\n    os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n    + \"/test_data/\"\n    + FILE_NAME\n    + \"_processed.csv\"\n)\n\n\n@pytest.mark.parametrize(\n    \"expected_X,expected_y\",\n    [\n        (\n            {\n                \"MinTemp\": {0: 13.4, 1: 7.4},\n                \"MaxTemp\": {0: 22.9, 1: 25.1},\n                \"Rainfall\": {0: 0.6, 1: 0.0},\n                \"WindGustSpeed\": {0: 44, 1: 44},\n                \"WindSpeed9am\": {0: 20, 1: 4},\n                \"WindSpeed3pm\": {0: 24, 1: 22},\n                \"Humidity9am\": {0: 71, 1: 44},\n                \"Humidity3pm\": {0: 22, 1: 25},\n                \"Pressure9am\": {0: 1007.7, 1: 1010.6},\n                \"Pressure3pm\": {0: 1007.1, 1: 1007.8},\n                \"Temp9am\": {0: 16.9, 1: 17.2},\n                \"Temp3pm\": {0: 21.8, 1: 24.3},\n                \"RainToday\": {0: 0, 1: 0},\n                \"WindGustDir_W\": {0: 1, 1: 0},\n                \"WindGustDir_WNW\": {0: 0, 1: 1},\n                \"WindDir9am_NNW\": {0: 0, 1: 1},\n                \"WindDir9am_W\": {0: 1, 1: 0},\n                \"WindDir3pm_WNW\": {0: 1, 1: 0},\n                \"WindDir3pm_WSW\": {0: 0, 1: 1},\n            },\n            [0, 0],\n        )\n    ],\n)\ndef test_get_variables(expected_X, expected_y):\n\n    # Open CSV as DF\n    data = pd.read_csv(PROCESSED_DATA_PATH)\n\n    # Run Function\n    X, y = get_variables(data, \"RainTomorrow\")\n\n    assert (X.to_dict(), y.to_list()) == (expected_X, expected_y)\n</code></pre> <p>It is a big part of MLOps to test models and data, but this is also heavily reliant on the type of project the team is working and on the type of data they are handling. It is part of the job of the team to define the scope of the tests and choosing the next steps of tests that fulfill the project requirements and try to get the maximum possible coverage at of the code, model behavior and data correctness.</p>"},{"location":"Deployment/","title":"Deployment with Watson Machine Learning","text":""},{"location":"Deployment/#what-is-watson-machine-learning","title":"What is Watson Machine Learning?","text":"<p>Watson Machine Learning (WML) is a service from the IBM Cloud suite that supports popular frameworks such as TensorFlow, PyTorch, and Keras to build and deploy models. Using this tool we can store, version and deploy models via online deployment. </p> <p>After creating and training a ML model we can upload it as an Asset in the Deployment Space, in the IBM Cloudpak. When we create a new deployment, we choose what model asset we want the deployment to reference:  IBM Dataplatform Deocumentation </p>"},{"location":"Deployment/#deployment-using-python-api","title":"Deployment using Python API","text":"<p>To deploy our ML model, we will use IBM's Watson Machine Learning, which will allow us to easily deploy the model as a web service. Since we want to automatize pipelines, we will be creating scripts using the WML Python API.</p> <p>Note</p> <p>The complete script can be found on our example repository</p> <p>The deployment scrip takes  the path to the trained model, the path to the root of the project containing the <code>metadata.yaml</code> file, and the credentials file.</p> <pre><code>python3 model_deploy_pipeline.py ./model_file ../path/to/project/ ../credentials.yaml\n</code></pre> <ol> <li> <p>Using these arguments constant variables are set         import os         import sys         import yaml</p> <pre><code>MODEL_PATH = os.path.abspath(sys.argv[1])\nPROJ_PATH = os.path.abspath(sys.argv[2])\nCRED_PATH = os.path.abspath(sys.argv[3])\nMETA_PATH = PROJ_PATH + \"/metadata.yaml\"\n</code></pre> </li> <li> <p>After that, the <code>yaml</code> files are loaded as dictionaries and the model is loaded (using either <code>joblib</code> or <code>pickle</code>).</p> <pre><code>with open(CRED_PATH) as stream:\n    try:\n        credentials = yaml.safe_load(stream)\n    except yaml.YAMLError as exc:\n        print(exc)\n\nwith open(META_PATH) as stream:\n    try:\n        metadata = yaml.safe_load(stream)\n    except yaml.YAMLError as exc:\n        print(exc)\n\nwith open(MODEL_PATH, \"rb\") as file:\n    # pickle_model = pickle.load(file)\n    pipeline = joblib.load(file)\n</code></pre> </li> <li> <p>The next step is to create an instance of the IBM Watson <code>client</code> , to do that the credential loaded above will be used and a default Deployment Space will be set using the ID contained in the credentials file, other constants will be set with information\u2019s regarding the model found on the <code>metadata</code> file.</p> <pre><code>from ibm_watson_machine_learning import APIClient\n\nwml_credentials = {\"url\": credentials[\"url\"], \"apikey\": credentials[\"apikey\"]}\n\nclient = APIClient(wml_credentials)\nclient.spaces.list()\n\nMODEL_NAME = metadata[\"project_name\"] + \"_\" + metadata[\"project_version\"]\nDEPLOY_NAME = MODEL_NAME + \"-Deployment\"\nMODEL = pipeline\nSPACE_ID = credentials[\"space_id\"]\n\nclient.set.default_space(SPACE_ID)\n</code></pre> </li> <li> <p>Before the deployment, we need to give Watson some characteristics from our model, such as name, type (in this case is  <code>scikit-learn_0.23</code>) and specifications of the instance that will run the micro-service. Next, the model is stored as an <code>Asset</code> on Watson ML.</p> <pre><code>model_props = {\n    client.repository.ModelMetaNames.NAME: MODEL_NAME,\n    client.repository.ModelMetaNames.TYPE: metadata[\"model_type\"],\n    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: client.software_specifications.get_id_by_name(\n        \"default_py3.7\"\n    ),\n}\n\nmodel_details = client.repository.store_model(model=MODEL, meta_props=model_props)\nmodel_uid = client.repository.get_model_uid(model_details)\n</code></pre> </li> <li> <p>Once completed, we'll give the deployment a name and then deploy the model using the store ID obtained in the previous step.</p> <pre><code>deployment_props = {\n    client.deployments.ConfigurationMetaNames.NAME: DEPLOY_NAME,\n    client.deployments.ConfigurationMetaNames.ONLINE: {},\n}\n\ndeployment = client.deployments.create(\n    artifact_uid=model_uid, meta_props=deployment_props\n)\n</code></pre> </li> <li> <p>Finally, the storage and deployment IDs are added or updated to in the <code>metadata.yaml</code> file.</p> <pre><code>deployment_uid = client.deployments.get_uid(deployment)\n\nmetadata[\"model_uid\"] = model_uid\nmetadata[\"deployment_uid\"] = deployment_uid\n\nf = open(META_PATH, \"w+\")\nyaml.dump(metadata, f, allow_unicode=True)\n</code></pre> </li> </ol>"},{"location":"Deployment/#accessing-model-predictions","title":"Accessing Model Predictions","text":"<p>Having deployed the model, we can access it's predictions by sending requests to an end-point or by using the Python <code>ibm_watson_machine_learning</code> library, where we can send either features for a single prediction or payloads containing multiple lines of a dataframe, for example.</p> <p>The payload body is made of the dataframe column names under the <code>\"fields\"</code> key and the features under <code>\"values\"</code> .</p> Watson API <pre><code>payload = {\n    \"input_data\": [\n        {\n            \"fields\": X.columns.to_numpy().tolist(),\n            \"values\": X.to_numpy().tolist(),\n        }\n    ]\n}\n\nresult = client.deployments.score(DEPLOYMENT_UID, payload)\n</code></pre> Requests <pre><code>import requests\n\nurl = \"https://us-south.ml.cloud.ibm.com/ml/v4/deployments?space_id=&lt;string&gt;&amp;tag.value=&lt;string&gt;&amp;asset_id=&lt;string&gt;&amp;version=2020-09-01\"\n\npayload = {\n    \"input_data\": [\n        {\n            \"fields\": X.columns.to_numpy().tolist(),\n            \"values\": X.to_numpy().tolist(),\n        }\n    ]\n}    \n\nheaders= {}\n\nresponse = requests.request(\"GET\", url, headers=headers, data = payload)\n\nprint(response.text.encode('utf8'))\n</code></pre> <p>The model response will contain the scoring result containing prediction and corresponding  probability. In the case of a binary classifier, the response will have the following format:</p> <pre><code>[1, [0.06057910314628456, 0.9394208968537154]],\n[1, [0.23434887273340754, 0.7656511272665925]],\n[1, [0.08054183674380211, 0.9194581632561979]],\n[1, [0.07877206037184215, 0.9212279396281579]],\n[0, [0.5719774367794239, 0.42802256322057614]],\n[1, [0.017282880299552716, 0.9827171197004473]],\n[1, [0.01714869904990468, 0.9828513009500953]],\n[1, [0.23952044576217457, 0.7604795542378254]],\n[1, [0.03055527110545664, 0.9694447288945434]],\n[1, [0.2879899631347379, 0.7120100368652621]],\n[0, [0.9639766912352016, 0.03602330876479841]],\n[1, [0.049694416576558154, 0.9503055834234418]],\n</code></pre> <p>Warning</p> <p>This consumes CUH. Watson Machine Learning CUH are used for running experiments, so there is a limit on how many times you can make requests to the model on a Free Tier.</p>"},{"location":"Deployment/#updating-the-model","title":"Updating the Model","text":"<p>Updating the asset containing the model and/or updating the deployment. </p> <p>Note</p> <p>The complete scripts for the deployment and model can be found on our template repository.</p> <ol> <li> <p>Firstly we need to update the model asset in WML by passing the new model as well as a name.</p> <pre><code>print(\"\\nCreating new version\")\n\npublished_model = client.repository.update_model(\n    model_uid=MODEL_GUID,\n    update_model=model,\n    updated_meta_props={\n        client.repository.ModelMetaNames.NAME: metadata[\"project_name\"]\n        + \"_\"\n        + metadata[\"project_version\"]\n    },\n)\n</code></pre> </li> <li> <p>After that a new revision can be created.</p> <pre><code>new_model_revision = client.repository.create_model_revision(MODEL_GUID)\n\nrev_id = new_model_revision[\"metadata\"].get(\"rev\")\nprint(\"\\nVersion:\", rev_id)\n\nclient.repository.list_models_revisions(MODEL_GUID)\n</code></pre> </li> <li> <p>Finally we can update the deployment.</p> <pre><code>change_meta = {client.deployments.ConfigurationMetaNames.ASSET: {\"id\": MODEL_GUID}}\n\nprint(\"Updating the following model: \")\nprint(client.deployments.get_details(DEPLOYMENT_UID))\n\nclient.deployments.update(DEPLOYMENT_UID, change_meta)\n</code></pre> </li> </ol>"},{"location":"Deployment/#model-rollback","title":"Model Rollback","text":"<p>We have previously created revisions of a model, to rollback the model version, we'll list all the revisions made.</p> <p>Note</p> <p>Complete script</p> <ol> <li> <p>Listing the revisions.</p> <pre><code>client.repository.list_models_revisions(MODEL_GUID)\n</code></pre> <p>Output:</p> <pre><code>--  -------------  ------------------------\nID  NAME           CREATED\n3   Rain_aus_v0.3  2021-03-31T18:28:07.771Z\n2   Rain_aus_v0.3  2021-03-31T18:28:07.771Z\n1   Rain_aus_v0.3  2021-03-31T18:28:07.771Z\n--  -------------  ------------------------\n</code></pre> </li> <li> <p>Now we can choose which revision we want to rollback to and then update the deployment referencing that revision ID.</p> <pre><code>MODEL_VERSION = input(\"MODEL VERSION: \")\n\nmeta = {\n    client.deployments.ConfigurationMetaNames.ASSET: {\n        \"id\": MODEL_GUID,\n        \"rev\": MODEL_VERSION,\n    }\n}\nupdated_deployment = client.deployments.update(\n    deployment_uid=DEPLOYMENT_UID, changes=meta\n)\n</code></pre> </li> <li> <p>Finally, we'll wait for the update to finish so we can see if it was successful.</p> <pre><code>status = None\nwhile status not in [\"ready\", \"failed\"]:\n    print(\".\", end=\" \")\n    time.sleep(2)\n    deployment_details = client.deployments.get_details(DEPLOYMENT_UID)\n    status = deployment_details[\"entity\"][\"status\"].get(\"state\")\n\nprint(\"\\nDeployment update finished with status: \", status)\n</code></pre> </li> </ol>"},{"location":"Infraestrutura/Auth/","title":"IAM Access","text":"<p>To manage users' access to the infrastructure resources is a good practice both as a server health safety and a cybersecurity matter, as it makes harder to gain admin access to those resources. In this section, it will be shown how to create your own API Key to use in many steps on this tutorial.</p>"},{"location":"Infraestrutura/Auth/#steps","title":"Steps","text":"<ol> <li>Login in your IBM cloud account and go to your dashboard.</li> <li>Click on the 'Manage' tab located in the top bar.</li> <li>On the left menu, go to 'API keys'.</li> <li>Finally, create your API key and keep it safe.</li> </ol> <p>Warning</p> <p>If by any means your API key leaks, it is very import to create a new one. Just follow the same steps above, but delete your old key.</p> <p>Notes</p> <p>For more information about account security access: What is IBM Cloud Identity and Access Management?</p>"},{"location":"Infraestrutura/Python/","title":"Managing the deployment space","text":"<p>The current IBM's terraform module is in development and some features are still missing. So we will use a Python script to manage the IBM Watson's deployment space, including create, delete and get the space_id, which will be very important as it is used in other scripts.</p>"},{"location":"Infraestrutura/Python/#authentication","title":"Authentication","text":"<p>The following code is used to authenticate to the provider, note that it uses the same environment variable as terraform. <pre><code>import os\nimport sys\nfrom pprint import pprint\nimport json\nfrom ibm_watson_machine_learning import APIClient\n\nTERRAFORM_OUTPUT = '.terraform/terraform.tfstate'\n\ndef authentication():\n\n    if os.getenv(\"IBMCLOUD_API_KEY\"):\n\n        wml_credentials = {\n            \"url\": \"https://us-south.ml.cloud.ibm.com\",\n            \"apikey\": os.environ.get(\"IBMCLOUD_API_KEY\"),\n        }\n        client = APIClient(wml_credentials)  # Connect to IBM cloud\n\n        return client\n\n    raise Exception(\"API_KEY environment variable not defined\")\n</code></pre></p>"},{"location":"Infraestrutura/Python/#terraform-output","title":"Terraform output","text":"<p>To create a deployment space, we need to get some metadata from the resources created by the terraform script. Here we use the output defined on terraform.</p> <pre><code>def terraform_output(terraform_path=TERRAFORM_OUTPUT):\n\n    output = dict(json.load(open(terraform_path)))['outputs']\n\n    cos_crn = output[\"cos_crn\"][\"value\"]\n    wml_crn = output[\"wml_crn\"][\"value\"][\"crn\"]\n    wml_name = output[\"wml_crn\"][\"value\"][\"resource_name\"]\n\n    state = {\n        \"cos_crn\" : cos_crn,\n        \"wml_name\": wml_name,\n        \"wml_crn\" : wml_crn\n    }\n    return state\n</code></pre>"},{"location":"Infraestrutura/Python/#creating-a-space","title":"Creating a space","text":"<p>Now with the metadata in hands, we can finally create a deployment space.</p> <pre><code>def create_deployment_space(\n    client, cos_crn, wml_name, wml_crn, space_name=\"default\", description=\"\"\n):\n\n    ## Project info\n    metadata = {\n        client.spaces.ConfigurationMetaNames.NAME: space_name,  \n        client.spaces.ConfigurationMetaNames.DESCRIPTION: description,\n        client.spaces.ConfigurationMetaNames.STORAGE: {\n            \"type\": \"bmcos_object_storage\",\n            \"resource_crn\": cos_crn,\n        },\n\n        ## Project compute instance (WML)\n        client.spaces.ConfigurationMetaNames.COMPUTE: { \n            \"name\": wml_name,\n            \"crn\": wml_crn,\n        },\n    }\n\n    space_details = client.spaces.store(meta_props=metadata)  # Create a space\n    return space_details\n</code></pre>"},{"location":"Infraestrutura/Python/#get-the-space_id","title":"Get the space_id","text":"<p>With the space created, now we have the space_id, the following function is used to retrieve it. This info will be used on other scripts that will be shown on next pages.</p> <pre><code>def update_deployment_space(client, new_name, space_id):\n\n    metadata = {client.spaces.ConfigurationMetaNames.NAME: new_name}\n\n    space_details = client.spaces.update(space_id, changes=metadata)\n    return space_details\n</code></pre> <p>Note</p> <p>To see the complete script click here</p>"},{"location":"Infraestrutura/Terraform/","title":"Setting Up the IBM Environment with Terraform","text":""},{"location":"Infraestrutura/Terraform/#introduction","title":"Introduction","text":"<p>Infrastructure as a code(IaC) is a process of managing and provisioning mechanisms for authenticating, planning and implementing servers and data centers in the cloud and private network without the need to manually configure everything through the provider's UI. IaC is a good practice that has been gaining attention since the DevOps culture started to grow and it is one of many processes that MLOps shares with it. For example, it makes easier to fast redeploy the infrastructure in a disaster scenario or an alternative plan to quickly change providers.</p> <p>For this example, we will use Terraform to deploy the needed IBM resources and a Python script to manage the deployment space inside the IBM Watson environment.</p>"},{"location":"Infraestrutura/Terraform/#what-is-terraform","title":"What is Terraform?","text":"<p>Terraform is an open-source infrastructure as a code software tool created by HashiCorp. It enables users to define and provision an infrastructure in a high-level configuration language. It saves the current state and any changes made on the script will only make the changes needed, e.g.,  change an instance name won't reset the instance itself or make any changes on other resources.</p>"},{"location":"Infraestrutura/Terraform/#requirements","title":"Requirements","text":"<p>We need to install the terraform and the IBM's module.</p>"},{"location":"Infraestrutura/Terraform/#terraform-script","title":"Terraform Script","text":"<pre><code>  # IBM's module.\n  # It will download the module when run \"terraform init\"\n\n  terraform {\n    required_providers {\n      ibm = {\n        source  = \"IBM-Cloud/ibm\"\n        version = \"~&gt; 1.12.0\"\n      }\n    }\n  }\n\n\n  provider \"ibm\" {}\n\n\n  # This will create a resource group\n  # It separates the resources used inside IBM's cloud\n\n  data \"ibm_resource_group\" \"group\" {\n    name = \"GROUP_NAME\"\n  }\n\n\n # This part will deploy a Watson machine learning resource\n\n  resource \"ibm_resource_instance\" \"wml\" {\n    name              = \"WML_NAME\"\n    service           = \"pm-20\"\n    plan              = \"lite\"\n    location          = \"us-south\"\n    resource_group_id = data.ibm_resource_group.group.id\n    tags              = [\"TEST\", \"TERRAFORM\"]\n\n  }\n\n\n # This deploys a IBM Cloud Object Storage resource\n\n  resource \"ibm_resource_instance\" \"cos\" {\n    name              = \"COS_NAME\"\n    service           = \"cloud-object-storage\"\n    plan              = \"standard\"\n    location          = \"global\"\n    resource_group_id = data.ibm_resource_group.group.id\n    tags              = [\"TERRAFORM\", \"TEST\"]\n\n  }\n</code></pre> <p>This script will create:</p> <ul> <li>Work group</li> <li>Watson Machine Learning resource</li> <li>IBM COS instance</li> </ul> <p>To install IBM's module run <code>terraform init</code>.</p>"},{"location":"Infraestrutura/Terraform/#authentication","title":"Authentication","text":"<p>Before you continue, you will need to create an API Key and assign it to an environment variable called IBMCLOUD_API_KEY.</p>"},{"location":"Infraestrutura/Terraform/#terraform-plan-and-apply","title":"Terraform Plan and Apply","text":"<p>If we run the command <code>terraform plan</code> the following output shows all the changes that will be made, in this case, create all the resources. <pre><code>Terraform will perform the following actions:\n\n  # ibm_resource_instance.cos will be created\n  + resource \"ibm_resource_instance\" \"cos\" {\n      + crn                     = (known after apply)\n      + dashboard_url           = (known after apply)\n      + extensions              = (known after apply)\n      + guid                    = (known after apply)\n      + id                      = (known after apply)\n      + location                = \"global\"\n      + name                    = \"TESTE_COS\"\n      + plan                    = \"standard\"\n      + resource_controller_url = (known after apply)\n      + resource_crn            = (known after apply)\n      + resource_group_id       = \"3f5502dbe92e42579cfdfec471f3ebd5\"\n      + resource_group_name     = (known after apply)\n      + resource_name           = (known after apply)\n      + resource_status         = (known after apply)\n      + service                 = \"cloud-object-storage\"\n      + status                  = (known after apply)\n      + tags                    = [\n          + \"TERRAFORM\",\n          + \"TEST\",\n        ]\n    }\n\n  # ibm_resource_instance.wml will be created\n  + resource \"ibm_resource_instance\" \"wml\" {\n      + crn                     = (known after apply)\n      + dashboard_url           = (known after apply)\n      + extensions              = (known after apply)\n      + guid                    = (known after apply)\n      + id                      = (known after apply)\n      + location                = \"us-south\"\n      + name                    = \"TESTE_TERRAFORM\"\n      + plan                    = \"lite\"\n      + resource_controller_url = (known after apply)\n      + resource_crn            = (known after apply)\n      + resource_group_id       = \"3f5502dbe92e42579cfdfec471f3ebd5\"\n      + resource_group_name     = (known after apply)\n      + resource_name           = (known after apply)\n      + resource_status         = (known after apply)\n      + service                 = \"pm-20\"\n      + status                  = (known after apply)\n      + tags                    = [\n          + \"TERRAFORM\",\n          + \"TESTE\",\n        ]\n    }\n\nPlan: 2 to add, 0 to change, 0 to destroy.\n</code></pre></p> <p>If everything is correct, then run <code>terraform apply</code> to create the infrastructure.</p>"},{"location":"Infraestrutura/Terraform/#outputs","title":"Outputs","text":"<p>After everything is set, terraform create a state file with the resources metadata. This file can be quite overwhelming  to work with and we will need some of this metadata. That's the use for an output file, when we run <code>terraform apply</code>, it detects this file and create a section on the state file with the information needed and it is easier to work with.</p> <p>The following script is an output file with the metadata that will be used on the next steps.</p> <pre><code>output \"cos_crn\" {\n  value = ibm_resource_instance.cos.crn\n}\n\noutput \"wml_name\" {\n  value = ibm_resource_instance.wml.name\n}\n\noutput \"wml_crn\" {\n  value = ibm_resource_instance.wml\n}\n</code></pre> <p>It is very straightforward and easy to read and understand. Note that, it is possible to create this file after everything is deployed and run a <code>terraform apply</code> again without worrying  about the instances.</p>"},{"location":"MLOps/CICDML/","title":"CI/CD for Machine Learning","text":"<p>Just like in DevOps, CI/CD is a method to make changes more frequently by automating the development stages. In machine learning(ML) this stages are different than a software development, a model depends not only on the code but also the data and hyperparameters, as well as deploying a model to production is more complex too.</p> <p></p>"},{"location":"MLOps/CICDML/#continuous-integration-ci","title":"Continuous Integration (CI)","text":"<p>Continuous integration in ML means that every time a code or data is updated the ML pipeline reruns, this is done in a way that everything is versioned and reproducible, so it is possible to share the codebase across projects and teams. Every rerun may consist in training, testing or generating new reports, making easier to compare between other versions in production.</p> <p>Note that, it is possible and recommended to run code tests too, for example, ensuring the code is in certain format, dataset values, such as NaN or wrong data types or functions outputs.</p> <p>Some examples of a CI workflow:</p> <ul> <li>running and versioning the training and evaluation for every commit to the repository.</li> <li>running and comparing experiment runs for each Pull Request to a certain branch.</li> <li>trigger a new run periodically.</li> </ul>"},{"location":"MLOps/CICDML/#continuous-deployment-cd","title":"Continuous Deployment (CD)","text":"<p>Continuous deployment is a method to automate the deployment of the new release to production, or any environment such as staging. This practice makes it easier to receive users' feedback, as the changes are faster and constant, as well as new data for retraining or new models.</p> <p>Some examples of CD workflow:</p> <ul> <li>Verify the requirements on the infrastructure environment before deploying it.</li> <li>Test the model output based on a known input.</li> <li>Load testing and model latency.</li> </ul>"},{"location":"MLOps/CICDML/#popular-cicd-tools-for-machine-learning","title":"Popular CI/CD tools for Machine Learning","text":"<p>There still aren't many alternatives for CI/CD in regards of Machine Learning. Regular CI/CD can be used if correctly implemented, such as running tests, deploy scripts, etc. focused on the MLOps environment. Although there are some tools preferred for most projects.</p> <p> Tools\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 License Developer Observations CML (Continuous Machine Learning) Open-source Iterative Most popular tools for CI/CD specific for Machine Learning. By the same developers of DVC, it can be integrated into it. Can be easily used with Github Actions or Gitlab CI/CD. Jenkins Open-source Jenkins CI Jenkins is a popular tool for regular CI/CD that can be used for Machine Learning after some configuration. It is a popular choice among some MLOps projects that intend to run tests on local hardware or heavily configured cloud services. <p></p> <ol> <li> <p>Image from Cloud HM blog. https://blog.cloudhm.co.th/ci-cd/\u00a0\u21a9</p> </li> </ol>"},{"location":"MLOps/Data/","title":"Versioning","text":""},{"location":"MLOps/Data/#data-and-model-versioning","title":"Data and Model Versioning","text":"<p>The use of code versioning tools is vital in the software development industry. The possibility of replicating the same code base so that several people can work on the same project simultaneously is a great benefit. In addition, versioning these bases allows them to work in different sections in an organized manner and without compromising the integrity of the code in production.</p> <p>As much as these tools solve several problems in software development, there are still issues in machine learning projects. Code versioning is still crucial, but when working on new experiments it's important to guarantee the same properties for data and models.</p> <p>In a machine learning project, data scientists are continuously working on the development of new models. This process relies on trying different combinations of data, parameters, and algorithms. It's extremely positive to create an environment where it's possible to go back and forth on older or new experiments. </p> <p></p>"},{"location":"MLOps/Data/#reproducibility","title":"Reproducibility","text":"<p>When discussing versioning, it's important to understand the term reproducibility. While versioning data, models, and code we are able to create a nice environment for data scientists to achieve the ultimate goal that is a good working model, there is a huge gap between this positive experiment to operationalize it. </p> <p>To guarantee that the experimentation of the data science team will become a model in the production for the project, it's important to make sure that key factors are documented and reusable. The following factors listed below were extracted from \"Introducing MLOps\" (Treveil and Dataiku Team 57) :</p> <ul> <li>Assumptions: Data Scientist's decisions and assumptions must be explicit.</li> <li>Randomness: Considering that some machine learning experiments contain pseudo-randomness, this needs to be in some kind of control so it can be reproduced. For example, using \"seed\".</li> <li>Data: The same data of the experiment must be available.</li> <li>Settings: Repeat and reproduce experiments with the same settings from the original.</li> <li>Implementation: Especially with complex models, different implementations can have different results. This is important to keep in mind when debugging.</li> <li>Environment: It's crucial to have the same runtime configurations among all data scientists.</li> </ul>"},{"location":"MLOps/Data/#popular-versioning-tools","title":"Popular Versioning Tools","text":"<p>Merging data, model, and code versioning isn't an easy task but fortunately, there are several tools being created or constantly updated to feel the need for data and model versioning. These tools can offer ways for transforming data with pipelines to configuring models.</p> <p>We listed below some of these tools available for usage:</p> Tool License Developer Observations IBM Watson ML Proprietary IBM Focused on model versioning DVC Open-source Iterative Popular lightweight open-source tool focused on data, model and pipeline versioning. Can be easily integrated with CML. Pachyderm Open-source Pachyderm Data platform built on Docker and Kubernetes. Focused on Version Control and automating workloads with parallelization. MLflow Open-source Databricks Popular tool for many parts of the Machine Learning lifecycle, including versioning of processes and experiments. Can be easily integrated with other tools Git LFS (Large File System) Open-source Atlassian, GitHub, and others Git extension that permits large files on Git repositories. Can be used to share large data files and models, using Git versioning. <ol> <li> <p>Image from DVC Documentation. https://dvc.org/doc/use-cases/versioning-data-and-model-files\u00a0\u21a9</p> </li> </ol>"},{"location":"MLOps/FeatureStore/","title":"Feature Storing","text":""},{"location":"MLOps/FeatureStore/#what-is-a-feature-store","title":"What is a Feature Store?","text":"<p>Feature Stores are components of data architecture that are becoming increasingly popular in the Machine Learning and MLOps environment. The goal of a Feature Store is to process data from various data sources at the same time and turn it into features, which will be consumed by the model training pipeline and the model serving. The concept of Feature Stores is novice and rapidly changing, therefore this page has the objective of showing the key features that are more common among the main Feature Stores in the market, but at the same time it is important to note that some of the tools and frameworks in the market might not comprehend all those exact characteristics in the same manner. </p> <p></p>"},{"location":"MLOps/FeatureStore/#why-it-matters","title":"Why it matters?","text":"<p>Feature Stores can be very useful for Machine Learning in production and are very reliable ways to manage features  for research and training using Offline Stores, as it is to manage the feeding of features to a model served in production using an Online Store. This data component can manage to comprehend a wide spectrum of different projects and necessities, some of which are seen below.</p>"},{"location":"MLOps/FeatureStore/#key-features","title":"Key Features","text":"<ul> <li>Enables features to be shared by multiple teams of Data Scientists working at the same time.</li> <li>Creates a reliable automated preprocess pipeline of large quantities of data.</li> <li>Can use and combine different data sources, such as data lakes, data warehouses and streaming of new data, all at once.</li> <li>Provides relevant and online features to a model in production.</li> <li>Can use a time windows system for Data Scientists to gather features from any point in time.</li> <li>Highly customizable for different model needs of consumption, such as batch or real-time predictions.</li> </ul>"},{"location":"MLOps/FeatureStore/#offline-store-vs-online-store","title":"Offline Store vs Online Store","text":"<p>Feature Stores combine multiple data sources and preprocess those into features, the main types of data are:</p> <ul> <li> <p>Batch Data: Usually coming from Data Lakes or Data Warehouses. Those are big chunks of data that have been stored in order to be used by models and are not necessarily updated in real-time. Example: Data from customers of a bank, such as age, country, etc.</p> </li> <li> <p>Real-time Data: Usually coming from Streaming and Log events. Those the online data that are constantly coming from sources like the events logged on a system. Example: A transaction in a bank is logged in real-time and fed to the Feature Store.</p> </li> </ul> <p>Those types of data are combined inside and form two types of stores: </p> <ul> <li>Offline Stores: Store composed of preprocessed features of Batch Data, used for building a historical source of features, that can be used by Data Scientists in the Model Training pipeline. With it's historical components, in most Feature Stores it can be used to provide a series of features at a given time frame or time point. It is normally stored in data warehouses, like IBM Cloud Object Storage, Apache Hive or S3, or in databases, like PostgreSQL, Cassandra and MySQL, but it can also be used in other kinds of systems, like HDFS.</li> <li>Online Stores: Store composed of data from the Offline Store combined with real-time preprocessed features from streaming data sources. It is built with the objective of being the most up-to-date collection of organized features, which can be used to feed the Model in Production with new features for prediction. It is normally stored in databases for rapid access, like MySQL, Cassandra, Redis, but it can be stored in more complex systems.  </li> </ul>"},{"location":"MLOps/FeatureStore/#common-architecture","title":"Common Architecture","text":""},{"location":"MLOps/FeatureStore/#popular-feature-stores","title":"Popular Feature Stores","text":"<p>Many companies relay on proprietary software to develop Feature Stores, since many try to make a component that fits into their exactly use-case, such a as:</p> Tools License Developer Observations Michelangelo Proprietary Uber Uber's platform for Machine Learning, focused on sharing feature pipelines with various teams. (Not open for public usage) Zipline Proprietary AirBnB Airbnb\u2019s Declarative Feature Engineering Framework (Not open for public usage) Metaflow Proprietary Netflix Netflix's human friendly Python/R library for Machine Learning. Has robust Feature Engineering and other attributes. (Open for public usage and  contribution) Feast Open-source Feast-dev, Tecton Popular open-source Feature Store. Very complete and competent data platform with Python, Spark and Redis. Integrates with many systems and is very customizable. Can be set up with Kubernetes. Hopsworks Open-source LogicalClocks Open-source Feature Store. Used by Amazon Sagemaker. Very hardware demanding. Butterfree Open-source QuintoAndar Open-source tool used for building Feature Stores using Python and Spark."},{"location":"MLOps/Monitoring/","title":"Continuous Monitoring","text":"<p>Machine Learning models are unique software entities as compared to traditional code and their performance can fluctuate over time due to changes in the data input into the model after deployment. So, once a model has been deployed, it needs to be monitored to assure that it performs as expected.</p> <p>It is also necessary to emphasize the importance of monitoring models in production to avoid discriminatory behavior on the part of predictive models. This type of behavior occurs in such a way that an arbitrary group of people is privileged at the expense of others and is usually an unintended result of how the data is collected, selected and used to train the models. </p> <p>Therefore, we need tools that can test and monitor models to ensure their best performance, in addition to mitigating regulatory, reputation and operational risks. </p>"},{"location":"MLOps/Monitoring/#what-to-monitor","title":"What to Monitor?","text":"<p>The main concepts that should be monitored are the following:</p> <ol> <li> <p>Performance: Being able to evaluate a model\u2019s performance based on a group of metrics and logging its decision or outcome can help give directional insights or compared with historical data. These can be used to compare how well different models perform and therefore which one is the best.</p> </li> <li> <p>Data Issues and Threats: Modern models are increasingly driven by complex feature pipelines and automated workflows that involve dynamic data that undergo various transformations. With so many moving parts, it\u2019s not unusual for data inconsistencies and errors to reduce model performance, over time, unnoticed. Models are also susceptible to attacks by many means such as injection of data.</p> </li> <li> <p>Explainability: The black-box nature of the models makes them especially difficult to understand and debug, especially in a production environment. Therefore, being able to explain a model\u2019s decision is vital not only for its improvement but also for accountability reasons, especially in financial institutions.</p> </li> <li> <p>Bias: Since ML models capture relationships from training data, it\u2019s likely that they propagate or amplify existing data bias or maybe even introduce new bias. Being able to detect and mitigate bias during the development process is difficult but necessary.</p> </li> <li> <p>Drift: The statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes, producing what is known as concept drift. </p> </li> </ol> <p>The following drawing<sup>1</sup> shows that the health of a Machine Learning system relies on hidden characteristics that are not easy to monitor therefore using the analogy of an iceberg. </p>"},{"location":"MLOps/Monitoring/#popular-serving-and-monitoring-tools","title":"Popular Serving and Monitoring Tools","text":"<p>Most tools used for serving Machine Learning models have monitoring tools. In many suites like IBM Watson, Microsoft Azure and Amazon Sagemaker there is components entirely dedicated to monitoring, like IBM Watson OpenScale. In the following table we can see some of the most popular monitoring tools for machine learning models.</p> Tools License Developer Observations IBM Watson OpenScale Proprietary IBM Monitors models deployed to IBM Watson Machine Learning. Monitors fairness, explainability and drift. Has tools for managing and correcting problems or inaccuracies in production. OpenShift Open-source Red Hat Kubernetes based system able to deploy various types of applications. It is platform agnostic and can be used for any type of application. Can be useful when a model is heavily integrated into a microservice  environment. Seldon Core Open-source SeldonIO Deploys models into microservices with Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests, Canaries and more. Platform agnostic and works with many Machine Learning frameworks Tensorflow Extended (TFX) Open-source Tensorflow Deploy Tensorflow Models as API and has monitoring capabilities. Arize AI Proprietary w/Community Edition Arize AI Monitors for performance, drift detection (including for embeddings), data quality checks, and model validation. <ol> <li> <p>Image from KDnuggets blog post \"A Machine Learning Model Monitoring Checklist: 7 Things to Track\". https://www.kdnuggets.com/2021/03/machine-learning-model-monitoring-checklist.html\u00a0\u21a9</p> </li> </ol>"},{"location":"MLOps/PipelineAutomation/","title":"Automation","text":""},{"location":"MLOps/PipelineAutomation/#why-automate-machine-learning","title":"Why automate Machine Learning?","text":"<p>The automation of machine learning pipelines is highly correlated with the maturity of the project. There are several steps between developing a model and deploying it, and a good part of this process relies on experimentation.</p> <p>Executing this workflow with less manual intervention as possible should result in:</p> <ul> <li>Fast deployments</li> <li>More reliable process</li> <li>Easier problem discovering</li> </ul>"},{"location":"MLOps/PipelineAutomation/#what-can-be-automated","title":"What can be automated?","text":"<p>In the table below, it's possible to find machine learning stages and what tasks may be automated:  ML Stages Tasks Data Engineering Data acquiring, validation, and processing Model Development Model training, evaluation, and testing Continuous Integration Build and testing Continuous Delivery Deployment new implementation of a model as a service Monitoring Setting alerts based on pre-defined metrics <p></p>"},{"location":"MLOps/PipelineAutomation/#levels-of-automation","title":"Levels of automation","text":"<p>Defining the level of automation has a crucial impact on the business behind the project. For example: data scientists spend a  good amount of time searching for good features, and this time can cost too much in resources which can impact directly the business return of investment(ROI).</p> <p>MLOps.org describes a three-level of automation in machine learning projects:</p> <ol> <li>Manual Process: Full experimentation pipeline executed manually using Rapid Application Development(RAD) tools, like Jupyter Notebooks. Deployments are also executed manually.</li> <li>Machine Learning automation: Automation of the experimentation pipeline which includes data and model validation.</li> <li>CI/CD pipelines: Automatically build, test and deploy of ML models and ML training pipeline components, providing a fast and reliable deployment.</li> </ol>"},{"location":"MLOps/PipelineAutomation/#common-tools-for-machine-learning-pipeline-automation","title":"Common Tools for Machine Learning Pipeline Automation","text":"<p>Currently there are many tools used to create and automate Machine Learning pipelines, experiments and any type of process. Below there is a list of some of their services.</p> <p> Tools License Developer Observations DVC Open-source Iterative DVC can be used to make Data Pipelines, which can be automated and reproduced. Very useful if already using DVC for Data and Model versioning. Easily configured and run. Language and framework agnostic. Tensorflow Extended (TFX) Open-source Tensorflow Used for production Machine Learning pipelines. Heavily integrated with Google and GCP. Only works with Tensorflow. Kubeflow Open-source Google, Kubeflow Kubeflow can build automated pipelines and experiments. Intended to build a complete end-to-end solution for Machine learning, being able to also serve and monitor models. Uses Kubernetes and is based on Tensorflow Extended. Works with Tensorflow and Pytorch. MLflow Open-source MLflow Project Open-source platform for the machine learning lifecycle. Can be used with Python, Conda and Docker. Large community. <p></p>"},{"location":"Openscale/","title":"Monitoring with IBM OpenScale","text":""},{"location":"Openscale/#setting-up-the-environment","title":"Setting Up the Environment","text":"<ol> <li> <p>Creating OpenScale service from the Services Catalog </p> </li> <li> <p>Creating Machine Learning Provider</p> <p>On the Machine Learning  Provider Tab, click on the <code>Add machine learning provider</code> button. </p> <p>2.1 Add a name and description.</p> <p></p> <p>2.2 Add connections  and Select Deployment Space Under <code>Service Provider</code>, select <code>Watson Machine Learning (V2)</code> from the dropdown. Next select the deployment space your model is located in. </p> </li> <li> <p>Adding to Dashboard     3.1 On the <code>Insights Dashboard</code>, click on the <code>Add to dashboard</code> button.          3.2 Next select the provider you just created, then select your model deployment and click on <code>Configure</code> and then <code>Configure Monitors</code>.</p> <p></p> <p>3.3 Select the data and algorithm  types, in our example it is a Binary Classification.</p> <p>3.4 The next step is selecting the training data that can be stored on a Db2 database or in IBM's Cloud Object Storage.</p> <p></p> <p>3.5 Now select the Label column (column you want to predict).</p> <p>3.6 Next we select all the features we want to include as well as indicate which ones are categorical. </p> <p>3.7 Here we can select Automatic Logging.</p> <p>3.8 Finally, we can select <code>prediction</code> and <code>probability</code> for the model output.</p> </li> <li> <p>Configuring Monitors     We can create monitors for <code>Fairness</code>,  <code>Quality</code>. <code>Drift</code> and <code>Explainability</code>.</p> <p>4.1 Fairness: The monitor checks your deployments for biases. It tracks when the model shows a tendency to provide a favorable (preferable) outcome more often for one group over another. </p> <p>We have to specify which values represent favorable outcomes and then select the features to monitor for bias, in our case we chose to monitor extreme temperatures in the <code>MinTemp</code> and <code>MaxTemp</code> columns.  </p> <p>4.2 Quality: This monitor evaluates how well the model predicts accurate outcomes that match labeled data. It identifies when model quality declines, so we can retrain your model if needed.</p> <p>We can set the Quality Threshold value, which Area under ROC, at 0.8.</p> <p>4.3 Drift: The drift evaluation measures drop in accuracy by estimating the drop in accuracy from a base accuracy score determined by the training data and also drops in data consistency, by estimating the drop in data consistency by comparing recent model transactions to the training data.</p> <p>We can set the Drift threshold as 20%.</p> <p>4.4 Explainability: This allows us to reveal which features contributed to the model\u2019s predicted outcome for a transaction and suggests what changes would result in a different outcome.</p> <p>We can set all features as controllable.</p> </li> </ol>"},{"location":"Openscale/#logging","title":"Logging","text":"<p>In the <code>Transactions</code> page, we can see informations about transactions, including a Timestamp, Prediction and Confidence.</p>"},{"location":"Openscale/#we-can-also-access-and-generate-logs-via-the-python-api","title":"We can also access and generate Logs via the Python API","text":"<ol> <li> <p>First we need to initialize  the Watson Machine Learning and OpenScale clients as well as the IAMAuthenticator.</p> <pre><code>service_credentials = {\n    \"apikey\": credentials[\"apikey\"],\n    \"url\": \"https://api.aiopenscale.cloud.ibm.com\",\n}\n\nDEPLOYMENT_UID = metadata[\"deployment_uid\"]\nMODEL_UID = metadata[\"model_uid\"]\nMODEL_NAME = metadata[\"project_name\"] + \"_\" + metadata[\"project_version\"]\nSPACE_ID = credentials[\"space_id\"]\nWOS_GUID = get_instance_guid(api_key=service_credentials[\"apikey\"])\nWOS_CREDENTIALS = {\n    \"instance_guid\": WOS_GUID,\n    \"apikey\": service_credentials[\"apikey\"],\n    \"url\": \"https://api.aiopenscale.cloud.ibm.com\",\n}\n\nif WOS_GUID is None:\n    print(\"Watson OpenScale GUID NOT FOUND\")\nelse:\n    print(WOS_GUID)\n\nwml_credentials = {\"url\": credentials[\"url\"], \"apikey\": credentials[\"apikey\"]}\n\nwml_client = ibm_watson_machine_learning.APIClient(wml_credentials)\n\nwml_credentials = {\n    \"url\": credentials[\"url\"],\n    \"apikey\": credentials[\"apikey\"],\n    \"instance_id\": \"wml_local\",\n}\n\nwml_client.set.default_space(SPACE_ID)\n\nauthenticator = IAMAuthenticator(apikey=credentials[\"apikey\"])\nwos_client = ibm_watson_openscale.APIClient(\n    authenticator=authenticator, \n    service_url=\"https://api.aiopenscale.cloud.ibm.com\")\n</code></pre> </li> <li> <p>Then we can get the model's scoring endpoint.</p> <pre><code>for deployment in wml_client.deployments.get_details()['resources']:\n    if DEPLOYMENT_UID in deployment['metadata']['id']:\n\n        scoring_endpoint = deployment['entity']['status']['online_url']['url']\n\nprint(scoring_endpoint)\n\nhttps://us-south.ml.cloud.ibm.com/ml/v4/deployments/e02e481d-4e56-470f-baa9-ae84a583c0a8/predictions\n</code></pre> </li> <li> <p>Here we display the OpenScale subscriptions.</p> <pre><code>wos_client.subscriptions.show()\n</code></pre> </li> <li> <p>Now we can load a dataset and then create the request body to make the predictions.</p> <pre><code>df_data = pd.read_csv(\"../data/weatherAUS_processed.csv\")\n\nX = df_data.iloc[:, :-1]\ny = df_data[df_data.columns[-1]]\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.01, random_state=1337\n)\n\npayload_scoring = {\n    \"input_data\": [\n        {\n            \"fields\": X.columns.to_numpy().tolist(),\n            \"values\": X_test.to_numpy().tolist(),\n        }\n    ]\n}\n</code></pre> </li> <li> <p>Then we send the request to our model.</p> <pre><code>scoring_response = wml_client.deployments.score(DEPLOYMENT_UID, payload_scoring)\n</code></pre> </li> <li> <p>After that, we use the <code>subscription_id</code> we got from step 3. we get the Payload data set ID.</p> <pre><code>subscription_id = 'bb7a45c3-15ad-4932-aeb8-8d32d54b8b05'\n\npayload_data_set_id = wos_client.data_sets.list(type=DataSetTypes.PAYLOAD_LOGGING, target_target_id=subscription_id, target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\n\nprint(\"Payload data set id:\", payload_data_set_id)\n\nPayload data set id: f4791725-24f8-4a00-9c13-b331ebca47f6\n</code></pre> </li> <li> <p>Now we can manually create logs with the predictions from our model and the data we sent in the request.</p> <pre><code>records = [PayloadRecord(request=payload_scoring, response=scoring_response, response_time=72)]\nstore_record_info = wos_client.data_sets.store_records(payload_data_set_id, records)\n</code></pre> </li> <li> <p>We also can do the same thing for <code>Feedback</code> datasets, which don't require the model prediction.</p> <pre><code>feedback_dataset = wos_client.data_sets.list(type=DataSetTypes.FEEDBACK, \n                                        target_target_id=subscription_id, \n                                        target_target_type=TargetTypes.SUBSCRIPTION).result\n\nfeedback_dataset_id = feedback_dataset.data_sets[0].metadata.id\nif feedback_dataset_id is None:\n    print(\"Feedback data set not found. Please check quality monitor status.\")\n    sys.exit(1)\n\ndata = X_test.to_dict('records')\n\nwos_client.data_sets.store_records(\n    feedback_dataset_id, \n    request_body=data, \n    background_mode=False,\n    header=True,\n    delimiter=',',\n    csv_max_line_length=1000)\n\nprint(wos_client.data_sets.get_records_count(data_set_id=feedback_dataset_id))\n</code></pre> </li> <li> <p>After that we can access these datasets as Pandas dataframes.</p> <pre><code>records2 = wos_client.data_sets.get_list_of_records(data_set_id=payload_data_set_id,output_type=ResponseTypes.PANDAS)\n\ndf = records2.result\n</code></pre> </li> <li> <p>Then we can use that Pandas dataframe to create plots or other forms of analysis.</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.hist(df.prediction_probability)\nplt.legend(title='Predictions Probability Histogram')\nplt.show()\n</code></pre> <p></p> </li> </ol>"},{"location":"Openscale/#evaluating-model","title":"Evaluating Model","text":"<p>On the main <code>Insights Dashboard</code> when click on our deployment, we can evaluate  or model by clicking on the <code>Actions</code> button on the top-right and then <code>Evaluate now</code> in dropdown, where we can import a test dataset by either directly uploading a <code>.csv</code> file or by using dataset or database stored in the IBM COS.</p> <p>After that, the metrics we defined for the monitors will be used to generate reports depicting our model's performance.</p>"},{"location":"Openscale/#explaining-predictions","title":"Explaining Predictions","text":"<p>Again, in the <code>Transactions</code> page, we can click on the <code>Explain</code> button, in the following page we can observe each features' relative weight indicating how strongly they influenced the model\u2019s predicted outcome.</p> <p></p> <p>In the <code>Inspect</code> tab, there is a table displaying the values each  feature would have to have to alter the prediction result, here we can also change the values by hand to see what the outcome would be.</p> <p></p>"},{"location":"Structure/project_structure/","title":"Tools and Project Structure","text":"<p>In the following sections we will go over the steps for the implementation of a MLOps Proof-of-Concept pipeline using IBM Watson tools and services. A template repository with a complete MLOps cycle: versioning data, generating reports on pull requests and deploying the model on releases with DVC and CML using Github Actions and IBM Watson as well as instructions to run the project can be found here.</p> <p>Note</p> <pre><code>We won't get into how to create predictive models or preprocessing data, since our main objective is to discuss MLOps and create a development cycle using those concepts.\n</code></pre> <p></p>"},{"location":"Structure/project_structure/#project-tools","title":"Project Tools","text":"<p>The main tools discussed in the guide are shown in the following table. As the guide is intended to be modular, a team can swap tools for others depending on the project necessities or preferences.</p> Tools Function Developer IBM Watson ML Deploying model as API IBM IBM Watson OpenScale Monitoring Model in production IBM DVC Data and Model Versioning Iterative CML Pipeline Automation Iterative Terraform Setups IBM infrastructure with script HashiCorp Github Code versioning Github Github Actions CI/CD Automation Github Pytest Python script testing Pytest-dev Pre-commit Running tests on local commit Pre-commit Cookiecutter Creating folder structure and files Cookiecutter"},{"location":"Structure/project_structure/#folder-structure","title":"Folder Structure","text":"<p>The above image is the project's folder structure, we'll talk about each specific part in further details trough out the guide.</p> <ul> <li> <p><code>data</code> , <code>models</code> and <code>results</code> contain files which are being stored and versioned by DVC.</p> </li> <li> <p><code>notebooks</code> contain Jupyter Notebooks used for the exploratory analysis, development of models, or data manipulation.</p> </li> <li> <p><code>src</code> contains scripts for training and evaluating the model as well as tests and scripts for pipelines and APIs.</p> </li> </ul> <p>This folder structure is going to be implemented in a blank project in Introduction/Starting a New Project with Cookiecutter</p>"},{"location":"Structure/project_structure/#requirements","title":"Requirements","text":"<p>The requirements file is a list of all of a project\u2019s dependencies and the specific version of each dependency, including the dependencies needed by the dependencies. It can also be used to create a virtual environment. This is extremely important to avoid conflicts between Python libraries and also ensure the experiments can be reproduced in different machines.</p>"},{"location":"Structure/project_structure/#metadata-file","title":"Metadata File","text":"<p>To keep track of the model information we have a <code>metadata.yaml</code> file, this helps with CI/CD and pipeline automation. Such as updating or deploying the model without the need of user input.</p> <pre><code>author: guipleite\ndatetime_creted: 29/03/2021_13:46:23:802394723\nmodel_type: scikit-learn_0.23\nproject_name: Rain_aus\nproject_version: v0.3\ndeployment_uid: e02e481d-4a56-470f-baa9-ae84a583c0a8\nmodel_uid: f29e4cfc-3aab-438a-b703-fabc265f43a3\n</code></pre>"},{"location":"Structure/project_structure/#using-jupyter-notebooks-vs-python-scripts","title":"Using Jupyter Notebooks vs. Python Scripts","text":"<p>The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. It is widely used in the fields of Data Science and Machine learning for its versatility in development and documentation of projects, however the usage of notebooks may cause some problems for our development cycle:</p> <ol> <li> <p>Versioning : Since notebooks source code are much more complex , we can't easily visualize the difference between versions using git. There are some tools that can help with that, however. </p> </li> <li> <p>Reproducibility : A great feature of notebooks is being able to run cells in a non-sequential order, but this is a big problem if we want to reproduce the code, since it's hard to know in what order or which cells where executed, this is especially bad if we want to automate pipeline.</p> </li> <li> <p>Standardized In/Out : By using scripts we can create pipelines with standardized entries and exits, therefore, we can create universal pipelines since no matter the model what it will receive and return will be in the same format.</p> </li> <li> <p>Access to Functions : In the <code>model.py</code> script, we define the <code>train</code> and <code>evaluate</code> function, where the model is declared and trained and the metrics for the evaluation are defined. These functions can be called by other scripts such as <code>train.py</code> and <code>evaluate.py</code> so we can create pipelines to train the model on a remote instance or evaluate an already trained model file in a consistent form.</p> <pre><code>def train(data, params):\n        ...\n        return pipeline, logs\n\ndef evaluate(data, pipeline, OUTPUT_PATH):\n        ...\n        return results\n</code></pre> </li> </ol> <p>In our project we choose to use scripts instead of Jupyter Notebooks for the reasons cited above, however notebooks could still be used as a form of experimentation of models or processes and the script as a more 'definitive' form.</p>"},{"location":"Structure/starting/","title":"Starting a New Project with Cookiecutter","text":""},{"location":"Structure/starting/#what-is-cookiecutter","title":"What is Cookiecutter?","text":"<p>Cookiecutter is a CLI tool that can be used to create projects based on templates. It can create folder structures and static files based on user input info on predefined questions. In this guide cookiecutter will create the project structure based on the MLOps Cookiecutter Template</p>"},{"location":"Structure/starting/#installing-cookiecutter","title":"Installing Cookiecutter","text":"<p>Cookiecutter is officially supported on Linux, MacOS and Windows. More info on installing it can be accessed on their documentation</p>"},{"location":"Structure/starting/#python","title":"Python","text":"<p><pre><code>pip install --user cookiecutter\n</code></pre> or</p> <pre><code>pip3 install --user cookiecutter\n</code></pre>"},{"location":"Structure/starting/#alternative-homebrew-macos-only","title":"Alternative: Homebrew (MacOS only)","text":"<pre><code>brew install cookiecutter\n</code></pre>"},{"location":"Structure/starting/#alternative-debianubuntu","title":"Alternative: Debian/Ubuntu","text":"<pre><code>sudo apt-get install cookiecutter\n</code></pre>"},{"location":"Structure/starting/#creating-a-new-project","title":"Creating a New Project","text":"<p>Now that cookiecutter is configured we can use the template to create a structured new project</p> <pre><code>cookiecutter https://github.com/mlops-guide/mlops-template.git\n</code></pre> <p>This should result in the following questions, which will be used to fill the project with info <pre><code>author [mlops-guide]:\nproject_name [Australia Weather Prediction]:\nproject_slug [australia_weather_prediction]:\nproject_version [v0.1]:\nmodel_type [scikit-learn_0.23]:\nSelect open_source_license:\n1 - MIT\n2 - BSD-3-Clause\n3 - No license file\nChoose from 1, 2, 3 [1]:\nuse_github_actions_for_CICD [y]:\nuse_pytest [y]:\nuse_black [y]:\n</code></pre></p>"},{"location":"Structure/starting/#basic-structure","title":"Basic Structure","text":"<p>After running cookiecutter, the project tree should be as the following. You can check this by running <code>$ tree</code> on Linux, using Finder on MacOS or File System on Windows. <pre><code>.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md\n\u251c\u2500\u2500 metadata.yaml\n\u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md\n\u251c\u2500\u2500 notebooks\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 results\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 scripts\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md\n    \u2514\u2500\u2500 tests\n        \u251c\u2500\u2500 README.md\n        \u2514\u2500\u2500 test_australia_weather_prediction.py\n\n7 directories, 11 files\n</code></pre></p>"},{"location":"Testes/","title":"Testes de Markdown","text":"<p>Essa p\u00e1gina \u00e9 dedicada a testes de Markdown e ferramentas complementares que possam ser interessantes ao guia. A partir dos testes \u00e9 poss\u00edvel exibir as ferramentas para os integrantes do grupo decidirem sobre sua utiliza\u00e7\u00e3o</p>"},{"location":"Testes/#ascii-cinema-melhorado","title":"ASCII Cinema Melhorado","text":""},{"location":"Testes/#como-usar","title":"Como usar","text":"<ul> <li>Grave o v\u00eddeo com o ASCII cinema record https://asciinema.org/</li> <li>Salve o o arquivo .cast na pasta ASCII_cinema</li> <li>Acesse dessa maneira: <pre><code>&lt;asciinema-player rows=18 theme=\"monokai\" cols=200 src=\"/ASCII_cinema/example.cast\"&gt;&lt;/asciinema-player&gt;\n</code></pre></li> </ul>"},{"location":"Versionamento/","title":"What is DVC?","text":"<p>In the MLOps Concepts section we were able to show how versioning data, models, and pipelines are so important in an ML project. In this section, we will cover DVC and how you can set up this tool to version your data, models and automatize pipelines.</p> <p>DVC, which goes by Data Version Control, is essentially an experiment management tool for ML projects. DVC software is built upon Git and its main goal is to codify data, models and pipelines through the command line.</p> <p>This is all possible because DVC replaces large files (such as datasets and ML models) with small metafiles that point to the original data. By doing that, it's possible to keep these metafiles along with the source code of the project in a repository while large files are kept in at a remote data storage.</p> <p></p> <p> DVC workflow from https://dvc.org </p> <p>Since DVC works on top of Git, its syntax and workflow are also similar so that you're able to treat data and model versioning just as you do with code. Although DVC can work stand-alone, it's highly recommended to work alongside Git.</p> <p>DVC can also manage the project's pipelines to make experiments reproducible for all members. These pipelines are lightweight and are created using dependency graphs. </p> <p>It's important to note that DVC is free, open-source and platform agnostic, so it runs with a diverse option of OS, programming languages and ML libraries.</p>"},{"location":"Versionamento/#install","title":"Install","text":"<p>DVC can be installed as a Python Library with pip package manager:</p> <pre><code>$ pip install dvc\n</code></pre> <p>Depending on which remote storage interface you're using, it's important to install optional dependencies(s3, azure, gdrive, gs, oss, ssh or all). In this  project  we are using S3 interface to connect with IBM Cloud Object Storage.</p> <pre><code>$ pip install \"dvc[s3]\"\n</code></pre> <p>It's also possible to install DVC using conda:</p> <pre><code>$ conda install -c conda-forge mamba\n$ mamba install -c conda-forge dvc\n$ mamba install -c conda-forge dvc-s3\n</code></pre>"},{"location":"Versionamento/#project-setup","title":"Project Setup","text":""},{"location":"Versionamento/#initialization","title":"Initialization","text":"<p>After installing DVC, you can go to your project's folder and initialize both Git and DVC:</p> <pre><code>$ git init\n$ dvc init\n</code></pre> <p>If you run <code>git status</code>, you can check DVC's initial config files that were created.</p> <p></p>"},{"location":"Versionamento/#remote-storage","title":"Remote Storage","text":"<p>Finally, we are going to setup our remote storage to keep our datasets and models. </p> <p>With <code>dvc remote add</code> you can point to your remote storage. <pre><code>$ dvc remote add -d remote-storage s3://bucket_namme/folder/\n</code></pre></p> <p>DVC uses by default AWS CLI to authenticate. IBM Cloud Object Storage works with S3 interface and also AWS, the only additional step here is to configure the IBM endpoint and configure the credentials.</p> <pre><code>$ dvc remote modify remote-storage endpointurl \\\n                    https://s3.us-south.cloud-object-storage.appdomain.cloud\n</code></pre> <p>To configure the credentials you can use default <code>~/.aws/credentials</code> file and just add a new profile, or point to a new credential path:</p> <p>Run: <pre><code>$ dvc remote modify myremote profile myprofile\n</code></pre> Modify <code>~/.aws/credentials</code> <pre><code>[default]\naws_access_key_id = ************\naws_secret_access_key = ************\n\n[myprofile]\naws_access_key_id = ************\naws_secret_access_key = ************\n</code></pre></p> <p>or</p> <pre><code>$ dvc remote modify credentialpath /path/to/creds\n</code></pre> <p>If you have any problems trying to configure your remote storage, go check DVC docs for remote command</p> <ul> <li> <p>Data Versioning</p> </li> <li> <p>Working with Pipelines </p> </li> </ul>"},{"location":"Versionamento/basic_dvc/","title":"Data Versioning","text":"<p>So now that your repository is initialized and connected with your remote storage, we can start to version your data.</p> <p>Warning</p> <p>This section will use the help of the template repository to show how to version data with DVC. Feel free to reproduce it with your own data files for your project.</p>"},{"location":"Versionamento/basic_dvc/#dvc-add","title":"dvc add","text":"<p>Suppose you have downloaded our weatherAUS.csv or another file inside your data folder and you want to add this file under the data version control of your project. The first step is to put this file under DVC local control and DVC cache by running:</p> <pre><code>$ dvc add data/weatherAUS.csv\n</code></pre> <p><code>dvc add</code> works the same way <code>git add</code> command. Your dataset is now under DVC local control and DVC cache(which is by default local but can be configured to be shared). But now, you need to put your data under Git version control with <code>git add</code>. Note that DVC created two files named <code>weatherAUS.csv.dvc</code> and <code>.gitignore</code> in your data folder. These files are responsible for codifying your data:</p> <ul> <li>weatherAUS.csv.dvc: This file points where your actual data is and every time that your data change, this file changes too.</li> <li>.gitignore: This file won't allow git to upload your data file to your repository. DVC creates automatically so you won't need to worry about it.</li> </ul> <p>These metafiles with <code>.dvc</code> extension are YAML files that contain some key-value pair information about your data or model. Here it's an example:</p> <pre><code>outs:\n  - md5: a304afb96060aad90176268345e10355\n    path: weatherAUS.csv\n</code></pre> <p>The md5 is a very common hash function that takes a file content and produces a string of thirty-two characters. So if you make just a small change in a data file or model controlled by DVC, the md5 hash will be recalculated and that's how your colleagues will keep track of what's new in your experiment.</p> <p>If  you are interested in all <code>.dvc</code> arguments, check out the official docs.</p>"},{"location":"Versionamento/basic_dvc/#dvc-checkout","title":"dvc checkout","text":"<p>To see how DVC has your data under control, you can remove your <code>weatherAUS.csv</code> . After that, try running <code>dvc checkout data/weatherAUS.csv.dvc</code> and you will see that your dataset is back!</p>"},{"location":"Versionamento/basic_dvc/#dvc-push","title":"dvc push","text":"<p>So now DVC and Git have your data under control. To finally allocated this data in your remote storage you need to run:</p> <pre><code>$ dvc push data/weatherAUS.csv\n</code></pre> <p>After that, your data is already in your remote storage and you just need to let Git know that by:</p> <pre><code>$ git commit -m 'data push'\n</code></pre>"},{"location":"Versionamento/basic_dvc/#dvc-pull","title":"dvc pull","text":"<p>Finally,  you can checkout to your experiment branch and pull your dataset related to that experiment:</p> <pre><code>$ git checkout branch_name\n</code></pre> <p>And to pull the dataset from your remote storage, just run:</p> <pre><code>$ dvc pull\n</code></pre>"},{"location":"Versionamento/pipelines_dvc/","title":"Working with Pipelines","text":"<p>After learning how to version data or models using DVC it's time to build your experiment pipeline.</p> <p>Let's assume that we are using the last section dataset as a data source for training a classification model. Let's also consider that we have three stages in this experiment:</p> <ul> <li>Preprocessing your data(extract features...)</li> <li>Train the model</li> <li>Evaluate the model</li> </ul> <p>Here you should have in hands our scripts: preprocess.py, train.py, evaluate.py and model.py.</p> <p>Warning</p> <p>Just as in the last section, we will use the help of the template repository to explain and build DVC's pipelines. Feel free to use your scripts and create specific pipelines for your project needs.</p>"},{"location":"Versionamento/pipelines_dvc/#creating-pipelines","title":"Creating pipelines","text":"<p>DVC builds a pipeline based on three components: Inputs, Outputs, and Command. So for the preprocessing stage, this would look like this:</p> <ul> <li>Inputs: weatherAUS.csv.csv and preprocess.py script</li> <li>Outputs: weatherAUS_processed.csv</li> <li>Command: python preprocess.py weatherAUS.csv</li> </ul> <p>So to create this stage of preprocessing, we use <code>dvc run</code>:</p> <pre><code>dvc run -n preprocess \\\n  -d ./src/preprocess_data.py -d data/weatherAUS.csv \\\n  -o ./data/weatherAUS_processed.csv \\\n  python3 ./src/preprocess_data.py ./data/weatherAUS.csv\n</code></pre> <p>We named this stage \"preprocess\" by using the flag <code>-n</code>. We also defined this stage inputs with the flag <code>-d</code> and the outputs with the flag <code>-o</code>. The command will always be the last piece of <code>dvc run</code> without any flag.</p> <p>Tip</p> <p>Output files are added to DVC control when reproducing a DVC stage. When finalizing your experiment remember to use <code>dvc push</code> to version not only the data used but those outputs generated from the experiment.</p> <p>The train stages would also be created using <code>dvc run</code>:</p> <pre><code>dvc run -n train \\\n  -d ./src/train.py -d ./data/weatherAUS_processed.csv -d ./src/model.py \\\n  -o ./models/model.joblib \\\n  python3 ./src/train.py ./data/weatherAUS_processed.csv ./src/model.py 200\n</code></pre> <p>Warning</p> <p>The number 200 at <code>dvc run</code> above is related to our script function. If your are using your own script just ignore it.</p> <p>At this point, you might have noticed that two new files were created: dvc.yaml and dvc.lock.  The first one will be responsible for saving what was described in each <code>dvc run</code> command. So if you wanna create or change a specific stage, it's possible to just edit dvc.yaml . Our current file would look like this:</p> <pre><code>stages:\n  preprocess:\n    cmd: python3 ./src/preprocess_data.py ./data/weatherAUS.csv\n    deps:\n    - ./src/preprocess_data.py\n    - data/weatherAUS.csv\n    outs:\n    - ./data/weatherAUS_processed.csv\n    - ./data/features.csv\n  train:\n    cmd: python3 ./src/train.py ./data/weatherAUS_processed.csv ./src/model.py 200\n    deps:\n    - ./data/weatherAUS_processed.csv\n    - ./src/model.py\n    - ./src/train.py\n    outs:\n    - ./models/model.joblib\n</code></pre> <p>The second file created is dvc.lock. This is also a YAML file and its function is similar to .dvc files. Inside, we can find the path and a hash code for each file of each stage so DVC can track changes. Tracking these changes is important because now DVC will know when a stage needs to be rerun or not.</p> <p>Your currently pipeline looks likes this:</p> <p></p>"},{"location":"Versionamento/pipelines_dvc/#saving-metrics","title":"Saving metrics","text":"<p>Finally, let's create our last stage so we can evaluate our model:</p> <pre><code>dvc run -n evaluate -d ./src/evaluate.py -d ./data/weatherAUS_processed.csv \\\n  -d ./src/model.py -d ./models/model.joblib \\\n  -M ./results/metrics.json \\\n  -o ./results/precision_recall_curve.png -o ./results/roc_curve.png \\\n  python3 ./src/evaluate.py ./data/weatherAUS_processed.csv ./src/model.py ./models/model.joblib\n</code></pre> <p>You might notice that we are using the -M flag instead of the -o flag. This is important because now we can keep the metrics generated by every experiment. If we run <code>dvc metrics show</code> we can see how good was the experiment:</p> <pre><code>$ dvc metrics show\nPath                  accuracy    f1       precision    recall        \nresults/metrics.json  0.84973     0.90747  0.8719       0.94607\n</code></pre> <p>Another import command is if we want to compare this experiment made in our branch to the model in production at the main branch we can do this by running <code>dvc metrics diff</code>:</p> <pre><code>$ dvc metrics diff\nPath                  Metric     Old      New      Change             \nresults/metrics.json  accuracy   0.84643  0.84973  0.0033\nresults/metrics.json  f1         0.9074   0.90747  8e-05\nresults/metrics.json  precision  0.85554  0.8719   0.01636\nresults/metrics.json  recall     0.96594  0.94607  -0.01987\n</code></pre> <p>The metrics configuration is saved at the dvc.yaml file.</p>"},{"location":"Versionamento/pipelines_dvc/#control-the-experiment","title":"Control the experiment","text":"<p>So now we have built a full machine learning experiment with three pipelines: </p> <p></p> <p>DVC uses a Direct Acyclic Graph(DAG) to organize the relationships and dependencies between pipelines. This is very useful for visualizing the experiment process, especially when sharing it with your team. You can check the DAG just by running <code>dvc dag</code>:</p> <pre><code>$ dvc dag\n\n +-------------------------+  \n | data/weatherAUS.csv.dvc |  \n +-------------------------+  \n               *              \n               *              \n               *              \n        +------------+        \n        | preprocess |        \n        +------------+        \n         **        **         \n       **            **       \n      *                **     \n+-------+                *    \n| train |              **     \n+-------+            **       \n         **        **         \n           **    **           \n             *  *             \n         +----------+         \n         | evaluate |         \n         +----------+   \n</code></pre> <p>If you want to check any changes to the project's pipelines, just run:</p> <pre><code>$ dvc status\n</code></pre>"},{"location":"Versionamento/pipelines_dvc/#reproducibility","title":"Reproducibility","text":"<p>Either if you changed one stage and need to run it again or if you are reproducing someone's experiment for the first time, DVC helps you with that:</p> <p>Run this to reproduce the whole experiment:</p> <pre><code>$ dvc repro\n</code></pre> <p>Or if you want just to reproduce one of the stages, just let DVC know that by:</p> <pre><code>$ dvc repro stage_name\n</code></pre> <p>Info</p> <p>If you are using <code>dvc repro</code> for a second time, DVC will reproduce only those stages that changes have been made.</p>"},{"location":"Workflow/","title":"Project Workflow","text":"<p>In this section, we present a workflow example on how to create a new experiment in the project. This is also a step-by-step guide to the video presented on the home page.</p> <p>Note</p> <pre><code>If you don't have a configured project already, go check out the [**Implementation Guide**](../Structure/project_structure/) first.\n</code></pre>"},{"location":"Workflow/#setup-your-environment","title":"Setup your environment","text":""},{"location":"Workflow/#cloning-your-repository-and-installing-dependencies","title":"Cloning your repository and installing dependencies","text":"<p>Let's start by cloning the project repository you will be working on. We need to make sure that we have installed all the dependencies necessary.</p> <p>Tip</p> <p>It's a good practice to create a virtual environment for each project you work. You can do that using venv or conda.</p> <p> <pre><code>git clone https://github.com/mlops-guide/dvc-gitactions.git\ncd dvc-gitactions\npip3 install -r requirements.txt\npre-commit install\n</code></pre></p>"},{"location":"Workflow/#updating-the-data-and-checking-the-project-specifications","title":"Updating the data and checking the project specifications","text":"<p>Your environment is ready! So now let's:</p> <ul> <li>Download the data</li> <li>Check the project pipelines</li> <li>Reproduce the main experiment(Optional)</li> <li>See current metrics</li> </ul> <p></p> <pre><code>dvc pull\ndvc dag\ndvc repro\ndvc metrics show\n</code></pre> <p>Info</p> <p>If you are confused with this DVC's commands, check out at the Implementation Guide the Versioning section.</p>"},{"location":"Workflow/#working-on-a-new-update","title":"Working on a New Update","text":"<p>Now that you are familiar with the project let's try a new experiment.</p>"},{"location":"Workflow/#edit-the-necessary-files","title":"Edit the necessary files","text":"<p>First, you should edit those files affected for your experiment. </p> <p>Here we are changing our model from a Logistic Regression classifier to a Random Forest at <code>model.py</code>.</p> model.py <pre><code>pipe = Pipeline(\n        [\n            (\"scaler\", StandardScaler()),\n-            (\"LR\", LogisticRegression(random_state=0, max_iter=num_estimators)),\n+             (\n+                 \"RFC\",\n+                 RandomForestClassifier(\n+                     criterion=\"gini\",\n+                     max_depth=10,\n+                     max_features=\"auto\",\n+                     n_estimators=num_estimators,\n+                 ),\n+             ),\n        ]\n    )\n</code></pre> <p>Info</p> <p>If you forgot how this project is organized and what each file is responsible for, go check out Tools and Project Structure.</p>"},{"location":"Workflow/#create-new-branch-and-reproduce-pipelines","title":"Create new branch and Reproduce pipelines","text":"<p>Second, let's create a new branch at our repository to version this new experiment. After that, we can reproduce the experiment and see how the new metrics compare to the current model metrics.</p> <p> <pre><code>git checkout -b RandomForestClassifier\ndvc repro\ndvc metrics diff\n</code></pre></p> <p>Note</p> <p>Observe that DVC avoided running the preprocess stage since our model change affected only the train and evaluate stages. To see more about DVC pipelines go check out Working with pipelines.</p> <p>Even though our experiment didn't improve our current model metrics, we will consider it good for production to demonstrate the rest of the workflow cycle.</p>"},{"location":"Workflow/#test-and-commit","title":"Test and Commit","text":"<p>Our experiment is ready, now let's:</p> <ul> <li>Format our code with black</li> <li>Upload to our branch in our Github repository</li> </ul> <p> <pre><code>black .\ngit add .\ndvc push\ngit commit -m \"Random Forest Experiment\"\ngit push origin RandomForestClassifier\n</code></pre></p> <p>Info</p> <p>The tests and format checking which ran after the commit command were executed by pre-commit</p>"},{"location":"Workflow/#experiment-deployment","title":"Experiment Deployment","text":""},{"location":"Workflow/#pull-request-and-automated-report","title":"Pull request and automated report","text":"<p>After uploading to our branch, we can now create a Pull Request at the Github Website.</p> <p>Info</p> <p>If forgot or want to know more about how this automated report was generated, go check out the Continuous Integration with CML and Github Actions</p>"},{"location":"Workflow/#release-and-watson-deployment","title":"Release and Watson Deployment","text":"<p>Supposing our experiment was merged to the main branch, we can consider it ready for deployment. To do so, let's release a new version of the project using Github Website.</p> <p>After releasing the new version, CML and Github Actions will trigger a script responsible for deploying our model to Watson ML.</p> <p>Info</p> <p>If forgot or want to know more how this deployment happens, go check out the Continous Delivery with CML, Github Actions and Watson ML</p>"},{"location":"Workflow/#monitoring","title":"Monitoring","text":"<p>Ending our workflow cycle, we can use IBM OpenScale tool to monitor the model in production.</p> <p>There we can create monitors for Drift, Accuracy and Fairness. We can also explain the model's predictions, understanding which feature had more weight in the decision and also see what changes would need to be made for the outcome to change.</p> <p>Info</p> <p>If forgot or want to know more how to monitor your model in production, go check out the Monitoring with IBM OpenScale</p>"}]}